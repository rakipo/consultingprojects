#!/usr/bin/env python3
"""
pipeline/ingest.py
PostgreSQL to Neo4j via Claude transformation pipeline
"""

import json
import os
import re
import psycopg2
from neo4j import GraphDatabase
import requests
from typing import List, Dict, Any
from dataclasses import dataclass
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class GraphData:
    nodes: List[Dict[str, Any]]
    relationships: List[Dict[str, Any]]

class GraphIngestion:
    def __init__(self):
        self.pg = psycopg2.connect(
            host=os.getenv('PG_HOST'),
            port=os.getenv('PG_PORT'),
            database=os.getenv('PG_DATABASE'),
            user=os.getenv('PG_USER'),
            password=os.getenv('PG_PASSWORD')
        )
        self.neo4j = GraphDatabase.driver(
            os.getenv('NEO4J_URI'),
            auth=(os.getenv('NEO4J_USER'), os.getenv('NEO4J_PASSWORD'))
        )
        self.claude_key = os.getenv('CLAUDE_API_KEY')
        
    def extract_articles(self, days: int = 7) -> List[Dict[str, Any]]:
        """Extract articles from PostgreSQL"""
        with self.pg.cursor() as cur:
            cur.execute("""
                SELECT id, title, content, tags, author, publish_date, domain
                FROM structured_content 
                WHERE publish_date > NOW() - INTERVAL %s
                ORDER BY publish_date DESC
            """, (f'{days} days',))
            
            return [{
                'id': str(row[0]),
                'title': row[1] or '',
                'content': row[2] or '',
                'tags': row[3] or [],
                'author': row[4] or '',
                'publish_date': row[5].strftime('%Y-%m-%d') if row[5] else '',
                'domain': row[6] or ''
            } for row in cur.fetchall()]
    
    def extract_entities(self, content: str) -> List[Dict[str, str]]:
        """Extract entities using simple regex patterns"""
        entities = []
        
        # Person names (Dr./Prof. + Name or Name + credentials)
        person_pattern = r'\b(?:Dr\.|Prof\.|Mr\.|Ms\.|CEO|CTO)\s+([A-Z][a-z]+ [A-Z][a-z]+)|([A-Z][a-z]+ [A-Z][a-z]+)(?:\s+(?:PhD|MD|CEO|CTO))'
        for match in re.finditer(person_pattern, content):
            name = match.group(1) or match.group(2)
            if name:
                entities.append({'name': name.strip(), 'type': 'PERSON'})
        
        # Organizations (Corp, Inc, LLC, University, etc.)
        org_pattern = r'\b([A-Z][a-zA-Z\s&]+(?:Corp|Inc|LLC|University|Institute|Foundation|Labs|Systems|Technologies))\b'
        for match in re.finditer(org_pattern, content):
            entities.append({'name': match.group(1).strip(), 'type': 'ORGANIZATION'})
        
        # Technology terms
        tech_pattern = r'\b(AI|ML|ChatGPT|GPT-4|machine learning|artificial intelligence|neural network|deep learning|blockchain|IoT|cloud computing)\b'
        for match in re.finditer(tech_pattern, content, re.IGNORECASE):
            entities.append({'name': match.group(1).lower(), 'type': 'TECHNOLOGY'})
        
        return list({entity['name']: entity for entity in entities}.values())  # Dedupe
    
    def transform_to_graph(self, article: Dict[str, Any]) -> GraphData:
        """Transform article via Claude API with fallback entity extraction"""
        
        prompt = f"""Convert this article to Neo4j graph format. Extract entities from content (people, organizations, technologies, concepts).

INPUT: {json.dumps(article)}

OUTPUT format (JSON only):
{{
  "nodes": [
    {{"label": "Article", "properties": {{"id": "{article['id']}", "title": "...", "publish_date": "...", "domain": "..."}}}},
    {{"label": "Topic", "properties": {{"name": "topic_name"}}}},
    {{"label": "Author", "properties": {{"name": "author_name"}}}},
    {{"label": "Entity", "properties": {{"name": "entity_name", "type": "PERSON|ORGANIZATION|TECHNOLOGY|CONCEPT"}}}}
  ],
  "relationships": [
    {{"from": "Article", "to": "Topic", "type": "TAGS"}},
    {{"from": "Article", "to": "Author", "type": "WRITTEN_BY"}},
    {{"from": "Article", "to": "Entity", "type": "MENTIONS"}}
  ]
}}

Return only valid JSON."""
        
        try:
            response = requests.post(
                "https://api.anthropic.com/v1/messages",
                headers={
                    "Content-Type": "application/json",
                    "x-api-key": self.claude_key,
                    "anthropic-version": "2023-06-01"
                },
                json={
                    "model": "claude-sonnet-4-20250514",
                    "max_tokens": 2000,
                    "messages": [{"role": "user", "content": prompt}]
                },
                timeout=30
            )
            response.raise_for_status()
            
            # Parse Claude response
            claude_text = response.json()['content'][0]['text']
            # Strip markdown if present
            claude_text = re.sub(r'```(?:json)?\n?', '', claude_text).strip()
            
            graph_data = json.loads(claude_text)
            return GraphData(graph_data['nodes'], graph_data['relationships'])
            
        except Exception as e:
            logger.warning(f"Claude API failed for {article['id']}: {e}. Using fallback.")
            return self._fallback_transform(article)
    
    def _fallback_transform(self, article: Dict[str, Any]) -> GraphData:
        """Fallback transformation without Claude"""
        nodes = [
            {
                "label": "Article",
                "properties": {
                    "id": article['id'],
                    "title": article['title'],
                    "publish_date": article['publish_date'],
                    "domain": article['domain']
                }
            }
        ]
        relationships = []
        
        # Add author
        if article['author']:
            nodes.append({
                "label": "Author",
                "properties": {"name": article['author']}
            })
            relationships.append({
                "from_id": article['id'],
                "to_name": article['author'],
                "from_label": "Article",
                "to_label": "Author",
                "type": "WRITTEN_BY"
            })
        
        # Add topics from tags
        for tag in article.get('tags', []):
            nodes.append({
                "label": "Topic",
                "properties": {"name": tag}
            })
            relationships.append({
                "from_id": article['id'],
                "to_name": tag,
                "from_label": "Article", 
                "to_label": "Topic",
                "type": "TAGS"
            })
        
        # Add extracted entities
        entities = self.extract_entities(article['content'])
        for entity in entities:
            nodes.append({
                "label": "Entity",
                "properties": {
                    "name": entity['name'],
                    "type": entity['type']
                }
            })
            relationships.append({
                "from_id": article['id'],
                "to_name": entity['name'],
                "from_label": "Article",
                "to_label": "Entity", 
                "type": "MENTIONS"
            })
        
        return GraphData(nodes, relationships)
    
    def ingest_graph(self, graph_data: GraphData) -> None:
        """Insert nodes and relationships into Neo4j"""
        
        def tx_ingest(tx):
            # Create nodes with proper MERGE
            for node in graph_data.nodes:
                label = node['label']
                props = node['properties']
                
                if label == "Article":
                    tx.run(
                        f"MERGE (n:{label} {{id: $id}}) SET n += $props",
                        id=props['id'], props=props
                    )
                elif label == "Author":
                    tx.run(
                        f"MERGE (n:{label} {{name: $name}})",
                        name=props['name']
                    )
                elif label == "Topic":
                    tx.run(
                        f"MERGE (n:{label} {{name: $name}})",
                        name=props['name']
                    )
                elif label == "Entity":
                    tx.run(
                        f"MERGE (n:{label} {{name: $name}}) SET n.type = $type",
                        name=props['name'], type=props.get('type', 'UNKNOWN')
                    )
            
            # Create relationships with proper matching
            for rel in graph_data.relationships:
                if hasattr(rel, 'get') and rel.get('from_id'):
                    # Fallback format
                    tx.run(f"""
                        MATCH (a:{rel['from_label']} {{id: $from_id}})
                        MATCH (b:{rel['to_label']} {{name: $to_name}})
                        MERGE (a)-[:{rel['type']}]->(b)
                    """, from_id=rel['from_id'], to_name=rel['to_name'])
                else:
                    # Claude format - match first node by ID, second by name
                    from_label, rel_type = rel['from'], rel['type']
                    to_label = rel['to']
                    
                    if from_label == "Article":
                        # Article relationships
                        if to_label == "Author":
                            tx.run(f"""
                                MATCH (a:Article), (b:Author)
                                WHERE a.id = $article_id AND b.name = $author_name
                                MERGE (a)-[:{rel_type}]->(b)
                            """, article_id=graph_data.nodes[0]['properties']['id'],
                                 author_name=[n['properties']['name'] for n in graph_data.nodes if n['label'] == 'Author'][0])
                        elif to_label == "Topic":
                            for topic_node in [n for n in graph_data.nodes if n['label'] == 'Topic']:
                                tx.run(f"""
                                    MATCH (a:Article {{id: $article_id}})
                                    MATCH (t:Topic {{name: $topic_name}})
                                    MERGE (a)-[:{rel_type}]->(t)
                                """, article_id=graph_data.nodes[0]['properties']['id'],
                                     topic_name=topic_node['properties']['name'])
                        elif to_label == "Entity":
                            for entity_node in [n for n in graph_data.nodes if n['label'] == 'Entity']:
                                tx.run(f"""
                                    MATCH (a:Article {{id: $article_id}})
                                    MATCH (e:Entity {{name: $entity_name}})
                                    MERGE (a)-[:{rel_type}]->(e)
                                """, article_id=graph_data.nodes[0]['properties']['id'],
                                     entity_name=entity_node['properties']['name'])
        
        with self.neo4j.session() as session:
            session.write_transaction(tx_ingest)
    
    def run_pipeline(self, days: int = 7) -> None:
        """Execute full pipeline"""
        logger.info(f"Starting ingestion pipeline for last {days} days")
        
        articles = self.extract_articles(days)
        logger.info(f"Extracted {len(articles)} articles")
        
        successful = 0
        failed = 0
        
        for article in articles:
            try:
                graph_data = self.transform_to_graph(article)
                self.ingest_graph(graph_data)
                logger.info(f"✓ Processed: {article['id']} - {article['title'][:50]}")
                successful += 1
            except Exception as e:
                logger.error(f"✗ Failed {article['id']}: {e}")
                failed += 1
        
        logger.info(f"Pipeline complete: {successful} successful, {failed} failed")
    
    def validate_schema(self) -> bool:
        """Validate Neo4j schema setup"""
        try:
            with self.neo4j.session() as session:
                # Check constraints exist
                result = session.run("SHOW CONSTRAINTS")
                constraints = [record['name'] for record in result]
                
                required = ['article_id', 'topic_name', 'author_name', 'entity_name']
                missing = [c for c in required if c not in constraints]
                
                if missing:
                    logger.error(f"Missing constraints: {missing}")
                    return False
                
                logger.info("Schema validation passed")
                return True
        except Exception as e:
            logger.error(f"Schema validation failed: {e}")
            return False
    
    def close(self):
        """Clean up connections"""
        self.pg.close()
        self.neo4j.close()

# Usage
if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()
    
    pipeline = GraphIngestion()
    
    # Validate schema first
    if not pipeline.validate_schema():
        logger.error("Schema validation failed. Run schema/init.cypher first.")
        exit(1)
    
    # Run pipeline
    pipeline.run_pipeline(days=7)
    pipeline.close()