# 03_execution Tree_neo4j Universal Content Intelligence Engine.txt

NEO4J UNIVERSAL CONTENT INTELLIGENCE ENGINE - EXECUTION TREE

Purpose: Built for Neo4j freelancer. No Claude runners. No database wrangling. Only focus: ingest from universal Postgres → create a fully queryable, taxonomy-driven knowledge graph that uncovers hidden content opportunities and powers data-driven content strategy across ANY content type (PDFs, articles, podcasts, videos, reports).

----

BUSINESS CONTEXT: Why This Matters

You're building a content intelligence engine that transforms raw content data into strategic business insights. This isn't just a database - it's a competitive advantage that reveals patterns humans can't spot manually.

What You're Competing Against:
- Manual research across different content types that takes hours per piece
- Gut feeling content decisions without cross-format data backing
- Missing opportunities because you can't see patterns across PDFs, articles, podcasts, and videos
- Reactive content strategy instead of proactive gap analysis across all content formats

What Success Looks Like:
- 10-minute research across any content type instead of hours of manual work
- Data-driven content calendars with proven opportunity scoring across formats
- Automated competitive intelligence that spots gaps across PDFs, articles, podcasts before competitors fill them
- Cross-format insights that reveal what works in articles vs PDFs vs audio content

----

INTELLIGENCE GOALS: What The Graph Should Uncover

Pattern Recognition
- Topics with high engagement but low content volume across any format (untapped opportunities)
- Content format preferences - what topics work better as PDFs vs articles vs podcasts
- Cross-format content gaps that appear across multiple successful pieces (validated demand)
- Author authority patterns in different domains and content types

Strategic Opportunities
- Underexplored topics with high search volume potential across content formats
- Content series potential based on natural topic relationships across PDFs, articles, podcasts
- Format optimization - topics that could be better served as different content types
- Cross-domain opportunities for expanding expertise into adjacent fields

Competitive Intelligence
- Framework patterns that work across multiple high-performing content pieces regardless of format
- Authority signals that indicate credible vs questionable sources
- Topic clustering that reveals natural content progression paths across different media
- Cross-format insights - what concepts work better in long-form vs short-form content

----

DAILY QUESTIONS: What You'll Query For

Content Planning (Weekly)
- "What are the top 5 high-urgency gaps in financial planning across all content types?"
- "Show me primary topics that have fewer than 3 pieces but high engagement rates"
- "Find content gaps that appear in high-authority sources - these are validated opportunities"
- "Which topics are covered well in PDFs but missing from articles?"

Competitive Research (Monthly)
- "Which topics do authoritative sources cover that I'm completely missing?"
- "What content formats work best for complex financial concepts vs simple tips?"
- "Show me secondary topics that could become primary topics based on gap frequency across all content"
- "Find insights that appear in academic papers but not in practical guides"

Format Optimization (Quarterly)
- "Map out a comprehensive guide using insights from PDFs + practical tips from articles"
- "Find 3 topics that frequently appear together across different content types"
- "Which tertiary topics are mentioned enough across formats to warrant dedicated content?"
- "What concepts are explained well in academic papers but poorly in practical content?"

Performance Optimization (Ongoing)
- "Show me insights with confidence > 0.8 from authoritative sources that I haven't addressed yet"
- "Find hook patterns that work well for technical content vs educational content"
- "Which misconception-type insights get mentioned most across high-authority content?"
- "What topics have high actionability but low content coverage across all formats?"

Cross-Format Intelligence (Strategic)
- "What's the content progression from academic research → practical guides → actionable tips?"
- "Which topics have high actionability in articles but appear complex in academic papers?"
- "Find content where gaps outnumber insights across different formats - expansion opportunities"
- "Show me topics that work well in long-form PDFs but are missing from short-form articles"

----

TECHNICAL REQUIREMENTS FOR FREELANCER

PHASE 0: UNIVERSAL DATABASE INTERFACE (Available)

Input is always this universal PostgreSQL schema — no changes allowed by freelancer.

Table structure (read-only):
- content_items → Universal content metadata (PDFs, articles, podcasts, videos, reports)
- content_insights → Structured insights with confidence
- content_gaps → Gaps/missing knowledge opportunities  
- content_topics → Hierarchy-based topic classifications
- content_hooks → Title/frame optimization (optional)

Freelancer doesn't touch this — just reads from it.

PHASE 1: BUILD NEO4J GRAPH SCHEMA

This is not just ingestion — it's semantic structuring.

Nodes to Create:
- (:Content) ← content_items table
- (:Insight) ← content_insights table  
- (:Gap) ← content_gaps table
- (:Topic) ← content_topics table
- (:Hook) ← content_hooks table (optional)

Relationships to Establish:

    (:Content)-[:CONTAINS]->(:Insight)
    (:Content)-[:HAS_GAP]->(:Gap)
    (:Content)-[:BELONGS_TO]->(:Topic)  
    (:Content)-[:HAS_HOOK]->(:Hook)

Critical Requirements:
- All merges must use primary keys
- Use MERGE not CREATE to avoid duplicates
- Normalize field types (e.g., confidence as float, hierarchy_level as enum)

PHASE 2: GRAPH TAXONOMY LOGIC

Hierarchies:
- (:Topic) uses:
    - name: "financial_planning"
    - hierarchy_level: "primary", "secondary", "tertiary"
    - domain: "finance", "legal", "technology", "healthcare"
    - audience_intent: "learning", "implementation", "research"

Priority Systems:
- Use (:Gap).urgency and (:Insight).confidence for downstream sorting and filtering
- All fields must be queryable in Cypher

PHASE 3: MCP SERVER INTEGRATION

Setup Neo4j MCP Cypher Server for natural language queries:

MCP Server Configuration:
```json
{
  "mcp_servers": {
    "neo4j-intelligence": {
      "command": "neo4j-mcp-server",
      "transport": "stdio",
      "env": {
        "NEO4J_URI": "bolt://localhost:7687",
        "NEO4J_USER": "neo4j",
        "NEO4J_PASSWORD": "password",
        "NEO4J_DATABASE": "content_intelligence"
      }
    }
  }
}
```

Docker MCP Server Setup:
```yaml
# docker-compose.yml addition
mcp-neo4j-server:
  image: neo4j/mcp-cypher-server:latest
  ports:
    - "8080:8080"
  environment:
    - NEO4J_URI=bolt://neo4j:7687
    - NEO4J_USER=neo4j
    - NEO4J_PASSWORD=password
    - MCP_SERVER_HOST=0.0.0.0
    - MCP_SERVER_PORT=8080
  depends_on:
    - neo4j
```

MCP Server Validation Script:
```python
# mcp_test.py
import requests
import json

def test_mcp_server():
    """Test MCP server connectivity and capabilities"""
    mcp_url = "http://localhost:8080"
    
    # Test server health
    try:
        response = requests.get(f"{mcp_url}/health")
        print(f"MCP Server Health: {response.status_code}")
    except Exception as e:
        print(f"MCP Server not accessible: {e}")
        return False
    
    # Test schema discovery
    try:
        schema_request = {
            "jsonrpc": "2.0",
            "method": "tools/list",
            "id": 1
        }
        response = requests.post(
            f"{mcp_url}/mcp",
            json=schema_request,
            headers={"Content-Type": "application/json"}
        )
        tools = response.json()
        print(f"Available MCP Tools: {tools}")
        return True
    except Exception as e:
        print(f"MCP Tools discovery failed: {e}")
        return False

if __name__ == "__main__":
    test_mcp_server()
```

Natural Language Query Interface:
```python
# mcp_query_interface.py
async def execute_natural_language_query(query: str):
    """
    Send natural language query to MCP server
    Examples:
    - "What are the top 5 content gaps in healthcare?"
    - "Show me insights with confidence > 0.8"
    - "Find topics that appear in PDFs but not articles"
    """
    mcp_request = {
        "jsonrpc": "2.0",
        "method": "tools/call",
        "params": {
            "name": "read_cypher_query",
            "arguments": {
                "instructions": query,
                "natural_language": True
            }
        },
        "id": 1
    }
    
    async with aiohttp.ClientSession() as session:
        async with session.post(
            "http://localhost:8080/mcp",
            json=mcp_request
        ) as response:
            result = await response.json()
            return result["result"]
```

PHASE 4: ENABLE CLAUDE-READY QUERIES

Freelancer must validate that the following queries return reliable, performant results:

Sample Queries (must test):

1. High Urgency Gaps:

    MATCH (c:Content)-[:HAS_GAP]->(g:Gap)
    WHERE g.urgency = "high"
    RETURN c.title, c.content_type, g.text
    ORDER BY c.publish_date DESC
    LIMIT 10

2. Insights by Confidence:

    MATCH (c:Content)-[:CONTAINS]->(i:Insight)
    WHERE i.confidence > 0.8
    RETURN c.title, c.content_type, i.text, i.type
    ORDER BY i.confidence DESC

3. Topics with Most Gaps:

    MATCH (c:Content)-[:BELONGS_TO]->(t:Topic), (c)-[:HAS_GAP]->(g:Gap)
    RETURN t.name, t.domain, count(g) as gap_count
    ORDER BY gap_count DESC
    LIMIT 5

4. Cross-Format Analysis:

    MATCH (c:Content)-[:HAS_HOOK]->(h:Hook)
    WHERE h.estimated_engagement = "high"
    RETURN c.content_type, h.text, h.category, count(*) as frequency
    ORDER BY frequency DESC

Freelancer must validate and index anything needed to support these.

5. Opportunity Discovery:

    // High-value gaps in popular topics across content types
    MATCH (c:Content)-[:BELONGS_TO]->(t:Topic), (c)-[:HAS_GAP]->(g:Gap)
    WHERE t.hierarchy_level = 'primary' AND g.urgency = 'high'
    RETURN t.name, t.domain, c.content_type, count(g) as gap_count
    ORDER BY gap_count DESC

6. Content Strategy:

    // Topics ready for cross-format development  
    MATCH (t1:Topic)<-[:BELONGS_TO]-(c:Content)-[:BELONGS_TO]->(t2:Topic)
    WHERE t1 <> t2 AND t1.hierarchy_level = 'primary'
    RETURN t1.name, c.content_type, collect(DISTINCT t2.name) as related_topics, count(c) as content_count
    HAVING content_count >= 3

7. Performance Intelligence:

    // High-confidence insights not yet exploited across formats
    MATCH (c:Content)-[:CONTAINS]->(i:Insight)
    WHERE i.confidence > 0.8 AND i.actionability = 'high'
    RETURN c.content_type, i.text, i.type, count(c) as frequency
    ORDER BY frequency DESC, i.confidence DESC

8. Cross-Format Optimization:

    // Content format patterns for engagement
    MATCH (c:Content)-[:HAS_HOOK]->(h:Hook)
    WHERE h.estimated_engagement = 'high'
    RETURN c.content_type, h.psychology_triggers, h.category, count(*) as usage_count
    ORDER BY usage_count DESC

----

FINAL DELIVERABLES

Freelancer must deliver:
- create_nodes.cypher → node + property schema
- create_relationships.cypher → relationship definitions
- query_tests.cypher → test suite for key queries  
- ingest_runner.py → PostgreSQL → Neo4j data pipeline
- mcp_server_setup.py → MCP Cypher server configuration
- mcp_test_queries.py → Natural language query validation
- requirements.txt → Python dependencies
- README.md:
    - how ingestion works
    - MCP server setup and testing
    - schema overview
    - example queries Claude will use
    - natural language interface examples
    - indexing strategy if any

----

FINAL TREE OVERVIEW (NEO4J + MCP)

    neo4j_graphrag/
    ├── create_nodes.cypher
    ├── create_relationships.cypher  
    ├── query_tests.cypher
    ├── ingest_runner.py
    ├── mcp_server_setup.py
    ├── mcp_test_queries.py
    ├── requirements.txt
    └── README.md